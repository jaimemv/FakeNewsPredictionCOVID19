{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"covid_data_extraction.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"13iRDeGWQp2cLhiAVkqQjysLePMEaNSLs","authorship_tag":"ABX9TyOiCWaTF1d65HCciPnXXaGl"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2r2D5Kt3BSCX"},"source":["## Fetch the data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ocp8IADZjicq","executionInfo":{"status":"ok","timestamp":1613106560571,"user_tz":-60,"elapsed":15066,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}},"outputId":"c27dad4f-aee5-415a-c805-91a7c71ec9a3"},"source":["#lime\n","!pip install lime -q\n","import lime\n","from lime.lime_text import LimeTextExplainer\n","import lime.lime_tabular\n","\n","# for hydrating tweet ids\n","!pip install twarc -q\n","\n","#utils\n","import os\n","import re\n","import json\n","import random\n","from datetime import datetime\n","from shutil import copyfile\n","import string\n","from string import ascii_letters\n","from collections import Counter, defaultdict\n","\n","#math, data\n","import math\n","import numpy as np\n","from numpy import inf\n","import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.tokenize import  word_tokenize \n","from textblob import TextBlob\n","\n","#plots, image\n","import cv2\n","from skimage import io\n","from PIL import Image\n","from wordcloud import WordCloud, STOPWORDS\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib import animation\n","%matplotlib inline\n","\n","#ml, nlp\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import export_graphviz, DecisionTreeClassifier\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.metrics import f1_score, accuracy_score,confusion_matrix\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.datasets import make_classification\n","from sklearn.linear_model import LogisticRegression\n","from nltk.stem.porter import PorterStemmer\n","from sklearn.pipeline import make_pipeline\n","from sklearn.decomposition import PCA, TruncatedSVD\n","\n","from IPython.display import Markdown, display\n","def printmd(string):\n","    display(Markdown(string))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K     |█▏                              | 10kB 19.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 25.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 13.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40kB 10.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 9.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 122kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 153kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 184kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 194kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 225kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 235kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 245kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 256kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 8.8MB/s \n","\u001b[?25h  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for twarc (setup.py) ... \u001b[?25l\u001b[?25hdone\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q2QZWKcH2tSu","executionInfo":{"status":"ok","timestamp":1613106596968,"user_tz":-60,"elapsed":9730,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}}},"source":["from IPython.display import clear_output\n","!rm -rf RawData\n","!rm -rf Junk\n","!rm -rf Classified\n","!gdown https://drive.google.com/uc?id=122tFro5g5u08M_DwLwYn5wDfHOR8BT48\n","!unzip RawData.zip\n","!rm RawData.zip\n","clear_output()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"qgifuvqz3PHy","executionInfo":{"status":"ok","timestamp":1613106596970,"user_tz":-60,"elapsed":4326,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}}},"source":["df = pd.read_csv('RawData/covid19_infodemic_english_data.tsv', sep='\\t')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nFzU7pEr3eUi","executionInfo":{"status":"ok","timestamp":1613106602502,"user_tz":-60,"elapsed":485,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}},"outputId":"df759859-d660-4247-a750-5e370e310f3a"},"source":["len(df)"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["504"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":649},"id":"bIvWZ-0t3iDI","executionInfo":{"status":"ok","timestamp":1613106603973,"user_tz":-60,"elapsed":523,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}},"outputId":"1bbdfc2e-8333-4780-b519-aea4b8dec3c4"},"source":["df_1 = df.loc[df['q1_label'] == 'yes']\n","df_1 = df.loc[(df['q2_label'] == '4_yes_probably_contains_false_info') | (df['q2_label'] == '5_yes_definitely_contains_false_info')]\n","df_1 = df.loc[(df['q5_label'] == 'yes_not_urgent') | (df['q5_label'] == 'yes_very_urgent')]\n","df_1.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>text</th>\n","      <th>q1_label</th>\n","      <th>q2_label</th>\n","      <th>q3_label</th>\n","      <th>q4_label</th>\n","      <th>q5_label</th>\n","      <th>q6_label</th>\n","      <th>q7_label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4</th>\n","      <td>1241447017945223169</td>\n","      <td>This is the face of someone who just spent 9 h...</td>\n","      <td>yes</td>\n","      <td>2_no_probably_contains_no_false_info</td>\n","      <td>4_yes_probably_of_interest</td>\n","      <td>1_no_definitely_not_harmful</td>\n","      <td>yes_not_urgent</td>\n","      <td>no_not_harmful</td>\n","      <td>no_not_interesting</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1240676575168921600</td>\n","      <td>So, the last week I have been battling COVID-1...</td>\n","      <td>yes</td>\n","      <td>2_no_probably_contains_no_false_info</td>\n","      <td>4_yes_probably_of_interest</td>\n","      <td>2_no_probably_not_harmful</td>\n","      <td>yes_not_urgent</td>\n","      <td>no_not_harmful</td>\n","      <td>yes_blame_authorities</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1240768501452697600</td>\n","      <td>He told wealthy friends. Then cashed out a coo...</td>\n","      <td>yes</td>\n","      <td>3_not_sure</td>\n","      <td>5_yes_definitely_of_interest</td>\n","      <td>5_yes_definitely_harmful</td>\n","      <td>yes_not_urgent</td>\n","      <td>yes_rumor_conspiracy</td>\n","      <td>yes_blame_authorities</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>1241337295606657033</td>\n","      <td>Korea finished developing the 10 minute Covid-...</td>\n","      <td>yes</td>\n","      <td>2_no_probably_contains_no_false_info</td>\n","      <td>5_yes_definitely_of_interest</td>\n","      <td>1_no_definitely_not_harmful</td>\n","      <td>yes_not_urgent</td>\n","      <td>no_not_harmful</td>\n","      <td>yes_discusses_action_taken</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>1241057919128526856</td>\n","      <td>THREAD: I mostly talk about movies on here, bu...</td>\n","      <td>yes</td>\n","      <td>2_no_probably_contains_no_false_info</td>\n","      <td>4_yes_probably_of_interest</td>\n","      <td>1_no_definitely_not_harmful</td>\n","      <td>yes_not_urgent</td>\n","      <td>no_not_harmful</td>\n","      <td>yes_discusses_action_taken</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               tweet_id  ...                    q7_label\n","4   1241447017945223169  ...          no_not_interesting\n","11  1240676575168921600  ...       yes_blame_authorities\n","13  1240768501452697600  ...       yes_blame_authorities\n","15  1241337295606657033  ...  yes_discusses_action_taken\n","19  1241057919128526856  ...  yes_discusses_action_taken\n","\n","[5 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wcD3Wa6N3q5i","executionInfo":{"status":"ok","timestamp":1613106613854,"user_tz":-60,"elapsed":497,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}},"outputId":"9dd0b384-10dc-434b-87f6-5cd197c10fc0"},"source":["# We only need the Tweet ID's\n","tweets_fake_1 = df_1['tweet_id']\n","print(\"This dataset contains\",df_1.shape[0], \"Tweet ID's\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["This dataset contains 160 Tweet ID's\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rQX4F8cV3wSA","executionInfo":{"status":"ok","timestamp":1613106618788,"user_tz":-60,"elapsed":763,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}}},"source":["def read_content_from_folder(dir_name): \n","  fake_claims = pd.read_csv(os.path.join(dir_name, 'ClaimFakeCOVID-19.csv'))['title']\n","  fake_claims = pd.DataFrame({'content':fake_claims})\n","  fake_claims_tweets = pd.read_csv(os.path.join(dir_name, 'ClaimFakeCOVID-19_tweets.csv'))['tweet_id']\n","  fake_news =  pd.read_csv(os.path.join(dir_name, 'NewsFakeCOVID-19.csv'))[['title', 'content']]\n","  fake_news['content'] = fake_news['title'].replace(np.nan, '', regex=True) + ' ' + fake_news['content'].replace(np.nan, '', regex=True)\n","  fake_news = pd.DataFrame({'content':fake_news['content']})\n","  fake_news_tweets = pd.read_csv(os.path.join(dir_name, 'NewsFakeCOVID-19_tweets.csv'))['tweet_id']\n","\n","  real_claims = pd.read_csv(os.path.join(dir_name, 'ClaimRealCOVID-19.csv'))['title']\n","  real_claims = pd.DataFrame({'content':real_claims})\n","  real_claims_tweets = pd.read_csv(os.path.join(dir_name, 'ClaimRealCOVID-19_tweets.csv'))['tweet_id']\n","  real_news =  pd.read_csv(os.path.join(dir_name, 'NewsRealCOVID-19.csv'))[['title', 'content']]\n","  real_news['content'] = real_news['title'].replace(np.nan, '', regex=True) + ' ' + real_news['content'].replace(np.nan, '', regex=True)\n","  real_news = pd.DataFrame({'content':real_news['content']})\n","  real_news_tweets = pd.read_csv(os.path.join(dir_name, 'NewsRealCOVID-19_tweets.csv'))['tweet_id']\n","\n","  return [fake_claims, fake_claims_tweets, fake_news, fake_news_tweets, real_claims, real_claims_tweets, real_news, real_news_tweets]\n","\n","l1 = read_content_from_folder('RawData/05-01-2020/')\n","l2 = read_content_from_folder('RawData/07-01-2020/')\n","\n","all_dfs = []\n","\n","# merging two lists, reset index\n","for i, dataframe in enumerate(l1):\n","  all_dfs.append(dataframe.append(l2[i], ignore_index=True))\n","\n","fc, fct, fn, fnt, rc, rct, rn, rnt = all_dfs"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"BmR5Zi284X4i","executionInfo":{"status":"ok","timestamp":1613106626424,"user_tz":-60,"elapsed":557,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}}},"source":["fct = tweets_fake_1.append(fct, ignore_index=True)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GE3fHNyy4cIh","executionInfo":{"status":"ok","timestamp":1613106632830,"user_tz":-60,"elapsed":698,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}},"outputId":"66a8e12b-bc5c-4991-9a5e-111fae595dca"},"source":["# code from: https://github.com/susanli2016/NLP-with-Python/blob/master/Fake_News_LogReg.ipynb\n","df = pd.read_csv('RawData/corona_fake.csv')\n","\n","df.loc[df['label'] == 'Fake', ['label']] = 'FAKE'\n","df.loc[df['label'] == 'fake', ['label']] = 'FAKE'\n","df.loc[df['source'] == 'facebook', ['source']] = 'Facebook'\n","df.text.fillna(df.title, inplace=True)\n","\n","df.loc[5,'label'] = 'FAKE'\n","df.loc[15,'label'] = 'TRUE'\n","df.loc[43,'label'] = 'FAKE'\n","df.loc[131,'label'] = 'TRUE'\n","df.loc[242,'label'] = 'FAKE'\n","\n","df = df.sample(frac=1).reset_index(drop=True)\n","df.title.fillna('missing', inplace=True)\n","df.source.fillna('missing', inplace=True)\n","\n","df['title_text'] = df['title'] + ' ' + df['text']\n","def preprocessor(text):\n","    \n","    text = re.sub('<[^>]*>', '', text)\n","    text = re.sub(r'[^\\w\\s]','', text)\n","    text = text.lower()\n","    return text\n","    \n","df['title_text'] = df['title_text'].apply(preprocessor)\n","df['label'].value_counts()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TRUE    586\n","FAKE    578\n","Name: label, dtype: int64"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"VlDsdhPy4lIz","executionInfo":{"status":"ok","timestamp":1613106638555,"user_tz":-60,"elapsed":472,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}}},"source":["df_t = pd.DataFrame({'content':df.loc[df['label']=='TRUE','title_text']})\n","df_f = pd.DataFrame({'content':df.loc[df['label']=='FAKE','title_text']})"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQH8KLcB4oPj","executionInfo":{"status":"ok","timestamp":1613106641016,"user_tz":-60,"elapsed":528,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}}},"source":["fn = fn.append(df_f, ignore_index=True)\n","rn = rn.append(df_t, ignore_index=True)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jys3TNwj4qee","executionInfo":{"status":"ok","timestamp":1613106645185,"user_tz":-60,"elapsed":518,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}}},"source":["bot_user_id = pd.read_csv(\"RawData/cresci_fake_users.txt\", header=None)\n","real_user_id = pd.read_csv(\"RawData/cresci_real_users.txt\", header=None)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jjMn1vah4s2-","executionInfo":{"status":"ok","timestamp":1613106646552,"user_tz":-60,"elapsed":506,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}},"outputId":"2b16e82e-a936-4aa2-da4c-893b8cbfcce6"},"source":["dataframes = [(\"fake_claims\", fc), (\"fake_claims_tweets\", fct), (\"fake_news\", fn), (\"fake_news_tweets\", fnt), \n","              (\"real_claims\", rc), (\"real_claims_tweets\", rct), (\"real_news\", rn), (\"real_news_tweets\", rnt),\n","              (\"bot_users\", bot_user_id), (\"real_users\", real_user_id)]\n","\n","[print(i[0], len(i[1])) for i in dataframes]\n","print()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["fake_claims 28\n","fake_claims_tweets 644\n","fake_news 1416\n","fake_news_tweets 10416\n","real_claims 338\n","real_claims_tweets 7795\n","real_news 3303\n","real_news_tweets 141548\n","bot_users 3455\n","real_users 3474\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OiEJ2VHr4z9F","executionInfo":{"status":"ok","timestamp":1613106659306,"user_tz":-60,"elapsed":8492,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}},"outputId":"456d490f-afd8-49d1-e3c8-afc35deccdad"},"source":["# you can ignore this variable\n","# this will always be False if you are not the author of this notebook\n","author = True\n","\n","# for temps files\n","!mkdir Junk\n","\n","if not author:\n","  tweets_fake = fnt.append(fct, ignore_index=True)\n","  tweets_true = rnt.append(rct, ignore_index=True)\n","\n","  # we combine all found tweet IDS\n","  print(len(tweets_fake), len(tweets_true))\n","\n","  with open(\"Junk/tweets_fake.txt\", \"w\") as f:\n","      for line in tweets_fake:\n","          f.write(str(line)+\"\\n\") \n","\n","  with open(\"Junk/tweets_real.txt\", \"w\") as f:\n","      for line in tweets_true:\n","          f.write(str(line)+\"\\n\") \n","\n","  copyfile(\"RawData/cresci_fake_users.txt\", \"Junk/cresci_fake_users.txt\")\n","  copyfile(\"RawData/cresci_real_users.txt\", \"Junk/cresci_real_users.txt\")\n","\n","  # Need a Twitter dev account and keys\n","  !twarc configure\n","\n","  !mkdir Classified\n","\n","  !twarc hydrate Junk/tweets_fake.txt > Classified/tweets_fake.jsonl\n","  !twarc hydrate Junk/tweets_real.txt > Classified/tweets_real.jsonl\n","  !twarc users Junk/cresci_real_users.txt > Classified/real_users.jsonl\n","  !twarc users Junk/cresci_fake_users.txt > Classified/fake_users.jsonl\n","else:\n","  # only the author can retrieve the prepopulated data from a personal Google drive folder\n","  !gdown https://drive.google.com/uc?id=1eSc4OM4-AOh0E829ShObkHk_NpiGGlL-\n","  !unzip Classified.zip\n","  !rm Classified.zip\n","\n","def read_json(filename):\n","  tweets = []\n","  with open(filename) as f:\n","    for line in f:\n","      tweets.append(json.loads(line))\n","  return tweets"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1eSc4OM4-AOh0E829ShObkHk_NpiGGlL-\n","To: /content/Classified.zip\n","68.1MB [00:01, 35.4MB/s]\n","Archive:  Classified.zip\n","   creating: Classified/\n","  inflating: Classified/tweets_real.jsonl  \n","  inflating: Classified/fake_users.jsonl  \n","  inflating: Classified/real_users.jsonl  \n","  inflating: Classified/tweets_fake.jsonl  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yCF9ztFI5Ol5","executionInfo":{"status":"ok","timestamp":1613106690916,"user_tz":-60,"elapsed":17173,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}},"outputId":"2d9d26f2-cd5b-4be6-e4ca-a16188b9b98f"},"source":["# these contain both Tweet data and user data\n","tweets_fake_ = read_json('Classified/tweets_fake.jsonl')\n","tweets_real_ = read_json('Classified/tweets_real.jsonl')\n","\n","# these only contain user data\n","users_fake_ = read_json('Classified/fake_users.jsonl')\n","users_real_ = read_json('Classified/real_users.jsonl')\n","\n","#sanity check\n","print(len(tweets_fake_))\n","print(len(tweets_real_))\n","print(len(users_fake_))\n","print(len(users_real_))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["10165\n","142761\n","2804\n","2892\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JXvfwAMU5Wrh","executionInfo":{"status":"ok","timestamp":1613106697056,"user_tz":-60,"elapsed":909,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}}},"source":["\n","tweet_columns = [\"id\", \"source\", \"full_text\", \"retweet_count\", \"favorite_count\", \"possibly_sensitive\", \"entities\", \"created_at\"]\n","user_columns = [\"id\", \"name\", \"screen_name\", \"has_extended_profile\", \"location\", \"description\", \"url\", \"protected\", \"followers_count\", \"friends_count\", \"listed_count\", \"created_at\", \"favourites_count\", \"verified\", \"statuses_count\", \"profile_image_url_https\", \"default_profile\", \"default_profile_image\"]\n","\n","# this function will be used to convert the json files that contain\n","# both the tweet level and user level data to a Pandas dataframe\n","def json_to_dataframe(json, amount):\n","  tweets = []\n","  users = []\n","  for i, t in enumerate(json):\n","    tweet_row = []\n","    user_row = []\n","    for _, tc in enumerate(tweet_columns):\n","      if (tc == \"possibly_sensitive\"):\n","        if \"possibly_sensitive\" in t:\n","          tweet_row.append(t[\"possibly_sensitive\"])\n","        else:\n","          tweet_row.append(False)\n","      else:\n","        tweet_row.append(t[tc])    \n","    for _, tu in enumerate(user_columns):\n","      user_row.append(t['user'][tu])\n","    tweets.append(tweet_row)\n","    users.append(user_row)\n","  df_t = pd.DataFrame(data= tweets, columns=tweet_columns)\n","  df_u = pd.DataFrame(data=users, columns=user_columns)\n","\n","  return df_t[0:amount], df_u[0:amount]\n","\n","# this function will be used to convert only the json files\n","# that contain user level data to a Pandas dataframe\n","def user_json_to_dataframe(json):\n","  users = []\n","  for i, u in enumerate(json):\n","    user_row = []\n","    for _, tu in enumerate(user_columns):\n","      user_row.append(u[tu])\n","    users.append(user_row)\n","  df_u = pd.DataFrame(data=users, columns=user_columns)\n","  return df_u"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"m8Lw0UVN7HLF","executionInfo":{"status":"ok","timestamp":1613106702374,"user_tz":-60,"elapsed":911,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}}},"source":["#https://towardsdatascience.com/automatically-detect-covid-19-misinformation-f7ceca1dc1c7\n","\n","#https://medium.com/pew-research-center-decoded/detecting-subjectivity-and-tone-with-automated-text-analysis-tools-5f0e662224b8\n","\n","def calculate_is_subjective(mined_data):\n","  def textblob_adj(text):\n","      blobed = TextBlob(text)\n","      counts = Counter(tag for word,tag in blobed.tags)\n","      adj_list = []\n","      adv_list = []\n","      adj_tag_list = ['JJ','JJR','JJS']\n","      adv_tag_list = ['RB','RBR','RBS']\n","      for (a, b) in blobed.tags:\n","        if b in adj_tag_list:\n","          adj_list.append(a)\n","        elif b in adv_tag_list:\n","          adv_list.append(a)\n","        else:\n","          pass\n","      return adj_list, adv_list, counts['JJ']+counts['JJR']+counts['JJS'], counts['RB']+counts['RBR']+counts['RBS'], counts['NNP']\n","    \n","  mined_data['subjective'] = 0\n","  mined_data['n_nnp'] = 0\n","\n","  for i, k in mined_data.iterrows():\n","    adj, adv, adj_c, adv_c, nnp_c = textblob_adj(k['full_text'])\n","    mined_data['n_nnp'].loc[i] = nnp_c\n","    if (adj_c >= 2 and adv_c >=1):\n","      mined_data['subjective'].loc[i] = 1\n","  return mined_data\n","\n","def preprocess_tweet(mined_data, keep_all=True):\n","  def add_amount_features(col_name, char):\n","    mined_data[col_name] = 0\n","    for i, s in enumerate(mined_data['full_text']):\n","      n_q = 0\n","      for k in s:\n","        if (k == char):\n","          n_q +=1\n","      mined_data[col_name].loc[i] = n_q\n","\n","  add_amount_features('n_question', '?')\n","  add_amount_features('n_exclaim', '!')\n","  add_amount_features('n_hash', '#')\n","  mined_data['n_uppercase'] = mined_data['full_text'].str.findall(r'[A-Z]').str.len()\n","  mined_data['length'] = mined_data['full_text'].str.len()\n","  mined_data['created_at_tweet'] = mined_data['created_at']\n","    \n","  mined_data['has_link'] = False\n","  for i, s in enumerate(mined_data['full_text']):\n","    mined_data.loc[i, 'has_link'] = ('t.co' in s) or ('http' in s)\n","  mined_data['has_link'] = mined_data['has_link'].astype(int)\n","  mined_data['possibly_sensitive'] = mined_data['possibly_sensitive'].astype(int)\n","  mined_data = calculate_is_subjective(mined_data)\n","\n","  return mined_data\n","\n","def preprocess_users(mined_data, keep_all=False):\n","  \n","  mined_data['bio_length'] = mined_data['description'].apply(len)\n","  mined_data['username_length'] = mined_data['screen_name'].apply(len)\n","  mined_data['name_length'] = mined_data['name'].apply(len)\n","  date_format = \"%a %b %d %H:%M:%S +%f %Y\" \n","  mined_data['age_account_days'] = 0\n","  for i, r in enumerate(mined_data['created_at']):\n","    a = datetime.now()\n","    b = datetime.strptime(r, date_format)\n","    mined_data['age_account_days'].iloc[i] = (a - b).days\n","\n","  mined_data['created_at_user'] = mined_data['created_at']\n","  \n","  mined_data['friends_count'] = mined_data['friends_count'].replace([0],1)\n","  mined_data['followers_friends_ratio'] = (mined_data['followers_count'] / mined_data['friends_count'])\n","  mined_data['age_account_days'] = mined_data['age_account_days'].replace([0],1)\n","  mined_data['likes_age_ratio'] = (mined_data['favourites_count'] / mined_data['age_account_days'])\n","\n","  mined_data['verified'] = mined_data['verified'].astype(int)\n","  mined_data['has_location'] = mined_data['location'] != ''\n","  mined_data['has_location'] = mined_data['has_location'].astype(int)\n","  mined_data['default_profile_image'] = mined_data['default_profile_image'].astype(int)\n","  mined_data['default_profile'] = mined_data['default_profile'].astype(int)\n","  mined_data['protected'] = mined_data['protected'].astype(int)\n","  mined_data['has_url'] = mined_data['url'].astype(str) != \"None\"\n","  mined_data['has_url'] = mined_data['has_url'].astype(int)\n"," \n","  return mined_data"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wSK6w4kr7OFq","executionInfo":{"status":"ok","timestamp":1613106780891,"user_tz":-60,"elapsed":74672,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}},"outputId":"0193a068-3d4c-4f89-88b2-0bcffeaf2f2f"},"source":["# these contain fake/real news and claims of Twitter users \n","# as well as the users that posted them\n","amount = 10000\n","tweets_fake, tweets_fake_users = json_to_dataframe(tweets_fake_, amount)\n","tweets_real, tweets_real_users = json_to_dataframe(tweets_real_, amount)\n","tweets_fake = preprocess_tweet(tweets_fake)\n","tweets_real = preprocess_tweet(tweets_real)\n","\n","# we preprocess the previous users and mine relevant features\n","tweets_fake_users = preprocess_users(tweets_fake_users)\n","tweets_real_users = preprocess_users(tweets_real_users)\n","\n","# these will be used to train the social bot detector \n","bots = preprocess_users(user_json_to_dataframe(users_fake_))\n","non_bots = preprocess_users(user_json_to_dataframe(users_real_))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  iloc._setitem_with_indexer(indexer, value)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IcYsM1xP7R2M","executionInfo":{"status":"ok","timestamp":1613106788095,"user_tz":-60,"elapsed":844,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}},"outputId":"bcb5c065-40f0-42ed-dd8b-677a9b030322"},"source":["\n","fake_news_articles = fn\n","real_news_articles = rn\n","print(len(fake_news_articles), \"Fake and\", len(real_news_articles), \"real news articles (text)\")\n","\n","fake_claims = fc\n","real_claims = rc\n","print(len(fake_claims), \"fake and\", len(real_claims), \"real website claims (text)\")\n","\n","print(len(tweets_fake), \"fake and\", len(tweets_real), \"real news and claims (tweets)\")\n","\n","print(\"The corresponding\", len(tweets_fake_users)+len(tweets_real_users) ,\"users that posted these fake real claims and news (users)\")\n","\n","#print(\"The corresponding\", len(tweets_fake_users_study)+len(tweets_real_users_study) ,\"users that posted these fake real claims and news (users) - version for user study\")\n"," \n","print(len(bots), \"bots and\", len(non_bots), \"real users (users)\")"],"execution_count":19,"outputs":[{"output_type":"stream","text":["1416 Fake and 3303 real news articles (text)\n","28 fake and 338 real website claims (text)\n","10000 fake and 10000 real news and claims (tweets)\n","The corresponding 20000 users that posted these fake real claims and news (users)\n","2804 bots and 2892 real users (users)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Tc0WEryC0yrf"},"source":["## Data mining"]},{"cell_type":"markdown","metadata":{"id":"-c6yNsc21Fty"},"source":["Add target attribute and concat all the DataFrames:"]},{"cell_type":"code","metadata":{"id":"1R8lcVa9D-ng","executionInfo":{"status":"ok","timestamp":1613106928152,"user_tz":-60,"elapsed":542,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}}},"source":["fake_news_articles[\"label\"] = 1\n","real_news_articles[\"label\"] = 0\n","fake_claims[\"label\"] = 1\n","real_claims[\"label\"] = 0\n","tweets_fake[\"label\"] = 1\n","tweets_real[\"label\"] = 0\n","\n","tweets_fake = tweets_fake[[\"full_text\", \"label\"]].copy()\n","tweets_real = tweets_real[[\"full_text\", \"label\"]].copy()\n","\n","dfs = [fake_news_articles,real_news_articles,fake_claims,real_claims,tweets_fake,tweets_real]\n","\n","# Rename features\n","for df in dfs:\n","  df.columns = [\"text\", \"target\"]\n","  \n","# Concat dfs\n","dataset = pd.concat(dfs)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ez8UsUyxKp2Z","executionInfo":{"status":"ok","timestamp":1613106936669,"user_tz":-60,"elapsed":514,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}}},"source":["dataset.drop_duplicates(inplace=True)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"lRteQ_ZZrPQE","executionInfo":{"status":"ok","timestamp":1613107103270,"user_tz":-60,"elapsed":716,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}},"outputId":"6a1ce7e1-c77b-40e8-c0b7-6b7312b2b442"},"source":["dataset.head()"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Facebook posts shared in at least three countr...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Wisconsin is Òclearly seeing a decline in COVI...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Facebook posts claim a child who is infected w...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>IndiaÕs Ministry of Home Affairs banning citiz...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>42 Democratic senators, plus two Independents,...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  target\n","0  Facebook posts shared in at least three countr...       1\n","1  Wisconsin is Òclearly seeing a decline in COVI...       1\n","2  Facebook posts claim a child who is infected w...       1\n","3  IndiaÕs Ministry of Home Affairs banning citiz...       1\n","4  42 Democratic senators, plus two Independents,...       1"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"lQfQWDaX1gzV"},"source":["### Text mining"]},{"cell_type":"code","metadata":{"id":"ca4Ln7e6EZJW"},"source":["contractions = { \n","\"ain't\": \"am not\",\n","\"aren't\": \"are not\",\n","\"can't\": \"cannot\",\n","\"can't've\": \"cannot have\",\n","\"'cause\": \"because\",\n","\"could've\": \"could have\",\n","\"couldn't\": \"could not\",\n","\"couldn't've\": \"could not have\",\n","\"didn't\": \"did not\",\n","\"doesn't\": \"does not\",\n","\"don't\": \"do not\",\n","\"hadn't\": \"had not\",\n","\"hadn't've\": \"had not have\",\n","\"hasn't\": \"has not\",\n","\"haven't\": \"have not\",\n","\"he'd\": \"he would\",\n","\"he'd've\": \"he would have\",\n","\"he'll\": \"he will\",\n","\"he's\": \"he is\",\n","\"how'd\": \"how did\",\n","\"how'll\": \"how will\",\n","\"how's\": \"how is\",\n","\"i'd\": \"i would\",\n","\"i'll\": \"i will\",\n","\"i'm\": \"i am\",\n","\"i've\": \"i have\",\n","\"isn't\": \"is not\",\n","\"it'd\": \"it would\",\n","\"it'll\": \"it will\",\n","\"it's\": \"it is\",\n","\"let's\": \"let us\",\n","\"ma'am\": \"madam\",\n","\"mayn't\": \"may not\",\n","\"might've\": \"might have\",\n","\"mightn't\": \"might not\",\n","\"must've\": \"must have\",\n","\"mustn't\": \"must not\",\n","\"needn't\": \"need not\",\n","\"oughtn't\": \"ought not\",\n","\"shan't\": \"shall not\",\n","\"sha'n't\": \"shall not\",\n","\"she'd\": \"she would\",\n","\"she'll\": \"she will\",\n","\"she's\": \"she is\",\n","\"should've\": \"should have\",\n","\"shouldn't\": \"should not\",\n","\"that'd\": \"that would\",\n","\"that's\": \"that is\",\n","\"there'd\": \"there had\",\n","\"there's\": \"there is\",\n","\"they'd\": \"they would\",\n","\"they'll\": \"they will\",\n","\"they're\": \"they are\",\n","\"they've\": \"they have\",\n","\"wasn't\": \"was not\",\n","\"we'd\": \"we would\",\n","\"we'll\": \"we will\",\n","\"we're\": \"we are\",\n","\"we've\": \"we have\",\n","\"weren't\": \"were not\",\n","\"what'll\": \"what will\",\n","\"what're\": \"what are\",\n","\"what's\": \"what is\",\n","\"what've\": \"what have\",\n","\"where'd\": \"where did\",\n","\"where's\": \"where is\",\n","\"who'll\": \"who will\",\n","\"who's\": \"who is\",\n","\"won't\": \"will not\",\n","\"wouldn't\": \"would not\",\n","\"you'd\": \"you would\",\n","\"you'll\": \"you will\",\n","\"you're\": \"you are\",\n","\"thx\"   : \"thanks\"\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uk_qx8M2KclI"},"source":["def remove_contractions(text):\n","    return contractions[text.lower()] if text.lower() in contractions.keys() else text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"izdRWP8yKkwB"},"source":["dataset[\"text\"] = dataset[\"text\"].apply(remove_contractions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHj9zA6YKS0I"},"source":["def clean_dataset(text):\n","    # Remove hashtag while keeping hashtag text\n","    text = re.sub(r'#','', text)\n","    # Remove HTML special entities (e.g. &amp;)\n","    text = re.sub(r'\\&\\w*;', '', text)\n","    # Remove tickers\n","    text = re.sub(r'\\$\\w*', '', text)\n","    # Remove hyperlinks\n","    text = re.sub(r'https?:\\/\\/.*\\/\\w*', '', text)\n","    # Remove whitespace (including new line characters)\n","    text = re.sub(r'\\s\\s+','', text)\n","    text = re.sub(r'[ ]{2, }',' ',text)\n","    # Remove URL, RT, mention(@)\n","    text=  re.sub(r'http(\\S)+', '',text)\n","    text=  re.sub(r'http ...', '',text)\n","    text=  re.sub(r'(RT|rt)[ ]*@[ ]*[\\S]+','',text)\n","    text=  re.sub(r'RT[ ]?@','',text)\n","    text = re.sub(r'@[\\S]+','',text)\n","    # Remove words with 4 or fewer letters\n","    text = re.sub(r'\\b\\w{1,4}\\b', '', text)\n","    #&, < and >\n","    text = re.sub(r'&amp;?', 'and',text)\n","    text = re.sub(r'&lt;','<',text)\n","    text = re.sub(r'&gt;','>',text)\n","    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n","    text= ''.join(c for c in text if c <= '\\uFFFF') \n","    text = text.strip()\n","    # Remove misspelling words\n","    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n","    # Remove emoji\n","    text = emoji.demojize(text)\n","    text = text.replace(\":\",\" \")\n","    text = ' '.join(text.split()) \n","    text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n","    # Remove Mojibake (also extra spaces)\n","    text = ' '.join(re.sub(\"[^\\u4e00-\\u9fa5\\u0030-\\u0039\\u0041-\\u005a\\u0061-\\u007a]\", \" \", text).split())\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pimVz5rWQJS2"},"source":["!pip install simpletransformers\n","!pip install emoji\n","!pip install itertools\n","clear_output()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RghbBSv9QBIw"},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import re\n","import sklearn\n","import itertools\n","import emoji\n","\n","# Debug: Metadata not found (AttributeError)\n","try:\n","  from simpletransformers.classification import ClassificationModel\n","except(AttributeError):\n","  !pip install simpletransformers\n","  from simpletransformers.classification import ClassificationModel\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rRrZWA1PwpF"},"source":["dataset[\"text\"] = dataset[\"text\"].apply(clean_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Qb3QFBNWT-m"},"source":["dataset.drop_duplicates(subset=[\"text\"], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9KmwZ9RRGwMr"},"source":["dataset.dropna(inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WMPJrRCTLXEB","executionInfo":{"status":"ok","timestamp":1613107977495,"user_tz":-60,"elapsed":3570,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}}},"source":["dataset.to_csv(\"covid_clean_dataset_09_02_21.csv\")\n","!cp covid_clean_dataset_09_02_21.csv /content/drive/MyDrive/Colab_Notebooks/Zenodo_DS_Project/"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R94tx9Ss51Ts"},"source":["## Clean ZENODO dataset"]},{"cell_type":"code","metadata":{"id":"MITr-mrz5103"},"source":["import pandas as pd\n","zenodo_tweets = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/Zenodo_DS_Project/hydrated_tweets.csv\", usecols=[\"text\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3_IWSu252A3"},"source":["contractions = { \n","\"ain't\": \"am not\",\n","\"aren't\": \"are not\",\n","\"can't\": \"cannot\",\n","\"can't've\": \"cannot have\",\n","\"'cause\": \"because\",\n","\"could've\": \"could have\",\n","\"couldn't\": \"could not\",\n","\"couldn't've\": \"could not have\",\n","\"didn't\": \"did not\",\n","\"doesn't\": \"does not\",\n","\"don't\": \"do not\",\n","\"hadn't\": \"had not\",\n","\"hadn't've\": \"had not have\",\n","\"hasn't\": \"has not\",\n","\"haven't\": \"have not\",\n","\"he'd\": \"he would\",\n","\"he'd've\": \"he would have\",\n","\"he'll\": \"he will\",\n","\"he's\": \"he is\",\n","\"how'd\": \"how did\",\n","\"how'll\": \"how will\",\n","\"how's\": \"how is\",\n","\"i'd\": \"i would\",\n","\"i'll\": \"i will\",\n","\"i'm\": \"i am\",\n","\"i've\": \"i have\",\n","\"isn't\": \"is not\",\n","\"it'd\": \"it would\",\n","\"it'll\": \"it will\",\n","\"it's\": \"it is\",\n","\"let's\": \"let us\",\n","\"ma'am\": \"madam\",\n","\"mayn't\": \"may not\",\n","\"might've\": \"might have\",\n","\"mightn't\": \"might not\",\n","\"must've\": \"must have\",\n","\"mustn't\": \"must not\",\n","\"needn't\": \"need not\",\n","\"oughtn't\": \"ought not\",\n","\"shan't\": \"shall not\",\n","\"sha'n't\": \"shall not\",\n","\"she'd\": \"she would\",\n","\"she'll\": \"she will\",\n","\"she's\": \"she is\",\n","\"should've\": \"should have\",\n","\"shouldn't\": \"should not\",\n","\"that'd\": \"that would\",\n","\"that's\": \"that is\",\n","\"there'd\": \"there had\",\n","\"there's\": \"there is\",\n","\"they'd\": \"they would\",\n","\"they'll\": \"they will\",\n","\"they're\": \"they are\",\n","\"they've\": \"they have\",\n","\"wasn't\": \"was not\",\n","\"we'd\": \"we would\",\n","\"we'll\": \"we will\",\n","\"we're\": \"we are\",\n","\"we've\": \"we have\",\n","\"weren't\": \"were not\",\n","\"what'll\": \"what will\",\n","\"what're\": \"what are\",\n","\"what's\": \"what is\",\n","\"what've\": \"what have\",\n","\"where'd\": \"where did\",\n","\"where's\": \"where is\",\n","\"who'll\": \"who will\",\n","\"who's\": \"who is\",\n","\"won't\": \"will not\",\n","\"wouldn't\": \"would not\",\n","\"you'd\": \"you would\",\n","\"you'll\": \"you will\",\n","\"you're\": \"you are\",\n","\"thx\"   : \"thanks\"\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L2-2d7Ic6FVi"},"source":["# Remove contractions: \n","def remove_contractions(text):\n","    return contractions[text.lower()] if text.lower() in contractions.keys() else text\n","\n","zenodo_tweets[\"text\"] = zenodo_tweets[\"text\"].apply(remove_contractions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mp4vpQUj6MdC"},"source":["def clean_dataset(text):\n","    # Remove hashtag while keeping hashtag text\n","    text = re.sub(r'#','', text)\n","    # Remove HTML special entities (e.g. &amp;)\n","    text = re.sub(r'\\&\\w*;', '', text)\n","    # Remove tickers\n","    text = re.sub(r'\\$\\w*', '', text)\n","    # Remove hyperlinks\n","    text = re.sub(r'https?:\\/\\/.*\\/\\w*', '', text)\n","    # Remove whitespace (including new line characters)\n","    text = re.sub(r'\\s\\s+','', text)\n","    text = re.sub(r'[ ]{2, }',' ',text)\n","    # Remove URL, RT, mention(@)\n","    text=  re.sub(r'http(\\S)+', '',text)\n","    text=  re.sub(r'http ...', '',text)\n","    text=  re.sub(r'(RT|rt)[ ]*@[ ]*[\\S]+','',text)\n","    text=  re.sub(r'RT[ ]?@','',text)\n","    text = re.sub(r'@[\\S]+','',text)\n","    # Remove words with 4 or fewer letters\n","    text = re.sub(r'\\b\\w{1,4}\\b', '', text)\n","    #&, < and >\n","    text = re.sub(r'&amp;?', 'and',text)\n","    text = re.sub(r'&lt;','<',text)\n","    text = re.sub(r'&gt;','>',text)\n","    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n","    text= ''.join(c for c in text if c <= '\\uFFFF') \n","    text = text.strip()\n","    # Remove misspelling words\n","    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n","    # Remove emoji\n","    text = emoji.demojize(text)\n","    text = text.replace(\":\",\" \")\n","    text = ' '.join(text.split()) \n","    text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n","    # Remove Mojibake (also extra spaces)\n","    text = ' '.join(re.sub(\"[^\\u4e00-\\u9fa5\\u0030-\\u0039\\u0041-\\u005a\\u0061-\\u007a]\", \" \", text).split())\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4dwaPBcM6bPT","executionInfo":{"status":"ok","timestamp":1612960164062,"user_tz":-60,"elapsed":7256,"user":{"displayName":"Jaime MV","photoUrl":"","userId":"16179252577833539524"}},"outputId":"02081914-5db2-4743-beff-41effa21a110"},"source":["!pip install simpletransformers\n","!pip install emoji\n","!pip install itertools"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: simpletransformers in /usr/local/lib/python3.6/dist-packages (0.60.6)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.22.2.post1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.19.5)\n","Requirement already satisfied: transformers>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (4.3.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.4.1)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.10.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2019.12.20)\n","Requirement already satisfied: streamlit in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.76.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.1.5)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.10.18)\n","Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.2.2)\n","Requirement already satisfied: tensorboardx in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.1)\n","Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (4.56.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.1.95)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->simpletransformers) (1.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (1.24.3)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=4.2.0->simpletransformers) (0.8)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=4.2.0->simpletransformers) (0.0.43)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers>=4.2.0->simpletransformers) (3.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=4.2.0->simpletransformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=4.2.0->simpletransformers) (20.9)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (2.8.1)\n","Requirement already satisfied: blinker in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (1.4)\n","Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (4.1.0)\n","Requirement already satisfied: astor in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.8.1)\n","Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.10.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (7.0.0)\n","Requirement already satisfied: pyarrow; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.14.1)\n","Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (4.2.1)\n","Requirement already satisfied: validators in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.18.2)\n","Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (5.1.1)\n","Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (1.5.1)\n","Requirement already satisfied: watchdog; platform_system != \"Darwin\" in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (1.0.2)\n","Requirement already satisfied: gitpython in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (3.1.13)\n","Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (0.6.0)\n","Requirement already satisfied: base58 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (2.1.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (7.1.2)\n","Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from streamlit->simpletransformers) (3.12.4)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2018.9)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (3.13)\n","Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (5.0.1)\n","Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (1.0.1)\n","Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (0.19.5)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (2.3)\n","Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (3.5.4)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (0.4.0)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (0.1.2)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (5.4.8)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from wandb->simpletransformers) (1.15.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=4.2.0->simpletransformers) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=4.2.0->simpletransformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=4.2.0->simpletransformers) (2.4.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.11.3)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.6.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.3)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.6/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.11.1)\n","Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from validators->streamlit->simpletransformers) (4.4.2)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from gitpython->streamlit->simpletransformers) (4.0.5)\n","Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (4.3.3)\n","Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (7.6.3)\n","Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.4.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit->simpletransformers) (53.0.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (1.1.1)\n","Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->gitpython->streamlit->simpletransformers) (3.0.5)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n","Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.5.0)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.5.1)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.0)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.1.2)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.5)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.6.1)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.8.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.18)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.1)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.1)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.7.1)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (22.0.2)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.5)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.6.1)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.9.2)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.4.3)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.6.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.3.0)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.4.4)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1)\n","Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (1.2.0)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement itertools (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for itertools\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mle7cUPi6i-N"},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import re\n","import sklearn\n","import itertools\n","import emoji"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bl-Ij6Qu60Ye"},"source":["zenodo_tweets[\"text\"] = zenodo_tweets[\"text\"].apply(clean_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eiieslCF7F66"},"source":["zenodo_tweets.drop_duplicates(subset=[\"text\"], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wiuOQ4_W7F6-"},"source":["zenodo_tweets.dropna(inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fGjgK717L4P"},"source":["zenodo_tweets.to_csv(\"zenodo_covid_dataset_1.csv\")\n","!cp zenodo_covid_dataset_1.csv /content/drive/MyDrive/Colab_Notebooks/Zenodo_DS_Project/"],"execution_count":null,"outputs":[]}]}